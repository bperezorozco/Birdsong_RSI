\documentclass[pdftex,11pt,a4paper]{article}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{subfiles}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{epstopdf}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{hyperref}
\usepackage{color}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue
}
\usetikzlibrary{shapes.geometric, arrows}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={210mm,297mm},
 left=30mm,
 right=30mm,
 top=30mm,
 bottom=35mm,
 }
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}
\newcommand*{\V}[1]{\mathbf{#1}}%
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\justif}[2]{&{#1}&\text{#2}}
\newcommand{\qedwhite}{\hfill \ensuremath{\Box}}
\newcommand\given[1][]{\:#1\vert\:}
\newcommand{\me}{\mathrm{e}}
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\newcommand{\Conv}{\mathop{\scalebox{1.5}{\raisebox{-0.2ex}{$\ast$}}}}%
\newcommand{\infdiv}{\infdivx}
\renewcommand{\qed}{\hfill\blacksquare}
\hyphenation{op-tical net-works semi-conduc-tor tech-no-lo-gy}


\begin{document}
\title{Learning relational structures from birdsong}
%\author{Bernardo P\'erez Orozco\\University of Oxford\\\texttt{ber@robots.ox.ac.uk} \and Davide Zilli \\University of Oxford\\\texttt{dzilli@robots.ox.ac.uk} \and Stephen Roberts\\University of Oxford\\\texttt{sjrob@robots.ox.ac.uk}}

\maketitle


\begin{abstract}
Birdsong is a rich means of communication that encapsulates behavioural patterns. As such, it can be analysed by means of statistical methods in order to unveil meaningful relationships that help to quantify an environment. In this work, we infer phylo-acoustic trees (relational structures built from acoustic similarity) for over 80 bird species in the British Isles. We characterise each bird species as a probabilistic model trained on birdsong excerpts and then use their pairwise similarity to build a phylo-acoustic tree by running Agglomerative Hierarchical Clustering. In order to achieve this, we define similarity metrics for Hidden Markov Models, hence implicitly defining a \emph{length-invariant} method to compare birdsong across different species. Finally, we show that the phylo-acoustic trees generated by our method do exhibit a clear community structure by comparing them with null hypothesis trees.
\end{abstract}


\section{Introduction}
\label{section_introduction}
The explosion of data in recent years has allowed scientists to gain insight in a diversity of domains. Furthermore, the widespread use of computational methods has in turn provided means of rapidly extracting conclusions from data. In this work, we tackle the problem of inferring phylo-acoustic trees of bird species by analysing their birdsong similarity. Crucially, this mathematical representation relies only on the validity of a formal analysis framework, and hence it unveils relations that go beyond empirical conclusions. 
\par Birdsong is a rich means of communication that encapsulates behavioural patterns (such as territory defence and mate attraction or competition \cite{Berwick2013, Naguib2014}) in a regular and hierarchical fashion \cite{Snowdon2013}, as shown in Figure \ref{fig_birdsong_structure}. Crucially, birdsong is learnt by repetition \cite{Berwick2013}, which implies that forced migration may push flocks to learn songs that do not necessarily come from their parents. Hence, building phylo-acoustic trees may further our understanding of how different species are related (even across different genera).
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/birdsong_structure}
\caption{Spectrogram of a birdsong recording of the species \emph{Periparus ater}. Note the regular structure of birdsong: syllables (thick boxes) are composed of motifs (thin boxes).}
\label{fig_birdsong_structure}
\end{figure}
\par Importantly, research in Zoology has pointed out the similarities between human speech and birdsong, e.g. we can think of both as ``complex, patterned vocalisations" \cite{Berwick2013,Naguib2014}. This remark enables us to analyse birdsong by means of some of the techniques from the field of Automatic Speech Recognition (ASR), which will be introduced in section \ref{section_model}. 
\par The rest of this article is organised as follows: first, in section \ref{section_review} we provide a literature review on automated birdsong analysis and its applications; then in section \ref{section_model}, we introduce the mathematical framework used to build phylo-acoustic trees; in section \ref{section_results}, we present our experiments and results; and in section \ref{section_conclusion} we present our conclusions and future work.

\section{Literature review}
\label{section_review}
Birdsong analysis has long been of interest in the research community because it encapsulates behavioural information that can be extracted by means of tools with a formal mathematical background. For example, birdsong analysis has been used extensively to perform bird species classification \cite{Chou2009,Wielgat2012,McIlraith1995,Lee2013,Franzen2003,Silla2013}, which consists in fitting a model that is able to tell what species unseen birdsong excerpts belong to.
\par More generally, bird species classification has also motivated the search for optimal feature representations of birdsong. Crucially, much research has pointed out the similarities between human speech and birdsong \cite{Snowdon2013,McIlraith1995}, which has enabled scientists to use methods from the speech recognition domain to analyse birdsong.
\par Some of the techniques frequently used in the literature to extract features from birdsong are: Mel-Frequency Cepstral Coefficients \cite{Silla2013,Wielgat2012,Stowell2014,Lopes2011}, raw Mel spectra \cite{Stowell2014} and wavelets \cite{Chou2009}. Other techniques used in the domain of Automated Speech Recognition include formant trajectory extraction \cite{Prica2010,Welling1998}.
\par One key remark is that these techniques are not length-invariant: they all produce feature representations that vary in length proportionally to the length of the original signal. This is a scalability issue that must be addressed in order to allow for faithful comparison across bird species. In particular, we seek to characterise bird species and not just birdsong recordings. This implies that recordings of radically different lengths should still be considered similar if they were both produced by the same species. This motivates the search for length-invariant comparison methods.
\par Methods used in the literature to handle comparisons between vectors of different length include Dynamic Time Warping (DTW) \cite{Jancovic2013,Muda2010} and zero-padding (i.e. ignoring the longer portion in the long recording, thus heavily affecting similarity for recordings with radically different durations); recall that we are concerned by characterising the source of the recordings, so neither DTW nor zero-padding address this issue directly. 
\par Nevertheless, bird species recognition is not the only task that has been studied; birdsong syllable segmentation, which consists in identifying recurring syllables in a song recordings, leaving aside all noise that is not part of it, has also benefited from the feature extraction techniques mentioned above \cite{Chou2008a}. 
\par However, there exists little work on environment quantification by means of birdsong statistical analysis. In \cite{Collins2009}, the authors have used ANOVA to analyse the relation between birdsong, migration strategy and divergent sexual selection. Another example is \cite{BolhuisJohanJ.andEveraertMartin2015}, in which the authors suggest a testable scenario for song evolution by describing acoustic and syntactic differences in bird behavioural traits, such as female responses to song complexity, effects of cross-fostering and identification of regional variations in birdsong.
\par Finally, to the authors' best knowledge, there is no work on building hierarchical similarity structures by means of birdsong analysis. Furthermore, these results can be used to improve existing applications of birdsong analysis, e.g. one such tree can be used as prior knowledge to build an \emph{in situ} bird species classifier.

\section{Model architecture}
\label{section_model}
In this section, we describe a procedure to build phylo-acoustic trees given a dataset $\mathcal{D}$ with birdsong recordings of different bird species from a set $X$. We are interested in finding a relation $R$ over pairs of elements in $X$, along with a scalar $s_{i, j} \in \mathbb{R}$ called the \emph{similarity} between species $s_i, s_j \in S$, i.e. $R \subseteq S \times S \times \mathbb{R}$. This relation can then be expressed as a symmetric matrix $A = (d_{i,j})$ and be used as input to build a hierarchical structure by means of Agglomerative Hierarchical Clustering (AHC). 
\par Due to the continuous nature of birdsong, recordings in $\mathcal{D}$ are rarely of the same length, and hence we would like to build a similarity matrix that is \emph{length-invariant}. We achieve this by characterising each bird species $s_k \in S$ as a statistical model, and then by defining a similarity metric between pairs of statistical models. In the rest of this section, we aim to describe how to train such models, and then explain how they can be used in conjunction with AHC to build phylo-acoustic trees.
\par In our work, we associate each bird species to a statistical model on the frequency components of their birdsong. In order to extract frequency features from birdsong, we use the Linear Predictive Coding (LPC) framework to compute the \emph{formant trajectories} of each birdsong recording. The LPC framework is based on the source-filter model of speech, which models acoustic signals shaped by the vocal tract, such as vowel sounds shaped by the larynx in the case of humans, and as birdsong, which is shaped by the syrinx.

\subsection{Feature extraction}
Many techniques exist in the literature to extract features from acoustic signals, most of which have been motivated by research in the field of human speech recognition. These include Mel-frequency Cepstral Coefficients \cite{Jurafsky2009, Chou2008a, Stowell2014, Gutierrez-Osuna2009}, which have also been used in combination with spherical K-means \cite{Stowell2014}, the MUSIC algorithm \cite{Evans, Kootsookos1999} and wavelets \cite{Gamulkiewicz2003, Chou2009}.
\par In this work, we are interested in extracting sequences of features that are compact and that have a solid mathematical framework based on how birdsong is produced. Crucially, characterising birdsong \emph{production} is actually enabling us to characterise its source, which are all the bird species we are analysing. Importantly, methods such as MFCCs are based on how sounds are perceived \cite{Muda2010}, rather than how they are produced. 
\par In order to meet this goal, we can use formant trajectory extraction, which is a technique based on the source-filter model of speech production \cite{Snell1993}. Formant trajectory extraction models the vocal tract of the individual producing speech (or birdsong) in a compact manner, and hence yields a formal mathematical underpinning to model birdsong production.
\par Consider a short excerpt of birdsong (20-40ms) $\V{s}$. Now, assume that this short section of birdsong recording $\V{s}$ can be approximated by a $p$-th order autoregressive model $\hat{\V{s}}$ with $\hat{s}_n = \sum_{i=1}^pa_is_{n-i}$. This prediction has an error $\V{e}$ given by:
\begin{align*}
e_n = s_n - \hat{s}_n = s_n - \sum_{i=1}^pa_is_{n-i}
\end{align*}
The error $\V{e}$ can also be seen as the output of an LTI system whose filter subtracts the best approximation $\hat{\V{s}}$ from $\V{s}$ \cite{Bello}, whose system function can be found by taking the z-transform of the equation above: 
\begin{align*}
\mathcal{Z}\{\V{e}\} &= \mathcal{Z}\{\V{s} - \hat{\V{s}}\}\\
E(z) &= \mathcal{Z}\{\V{s}\} - \mathcal{Z}\left\{ \left(\sum_{i=1}^pa_ix_{n-i}\right)_{n=1}^N\right\} \\
 &= S(z) - S(z)\sum_{i=1}^pa_iz^{-i}\\
 &= S(z)A(z)
\end{align*}
\par where $A(z) = 1 - \sum_{i=1}^pa_iz^{-i}$ is the system function. Now, consider the inverse LTI system with input $\V{e}$, output $\V{s}$ and system function $H(z)$. Then, $H(z)A(z) = 1$ and hence:
\begin{align*}
H(z) = \frac{1}{1-\sum_{i=1}^pa_iz^{-i}}
\end{align*}
where the $(a_i)_{i=1}^p$ are the coefficients of the p-th order autoregressive model associated to the birdsong approximation $\hat{\V{s}}$. Crucially, remark that this LTI system is fully characterised by the coefficients $a_i$. 
\par We now define the formant frequencies of the excerpt $\V{s}$ as the poles of the system function $H(z)$; intuitively, these can be thought of as the resonances of the vocal tract \cite{Prica2010} when generating birdsong. Now, given a pair of complex roots $re^{\pm\theta i}$, the formant frequency in Hertz associated to it is given by:
\begin{align*}
F = \frac{f_s}{2\pi}\theta
\end{align*}
where $f_s$ is the sampling frequency of the signal. Furthermore, its 3 dB bandwidth in Hertzis:
\begin{align*}
B = -\frac{f_s}{2\pi}\log{r}
\end{align*}
\par Another way of interpreting formants is as the peaks of the spectral envelope of the signal \cite{Darch}. This gives an intuition to the 3 dB bandwidth, which is simply the width of envelope exactly 3 dB below the peak. Smaller bandwidths hint at clearer, more characteristic formants. By framing the original birdsong recording into small excerpts and repeating the procedure above, we obtain a trajectory of formants extracted uniformly over small intervals of time. Figure \ref{fig_specformants} shows the trajectory of the first non-zero formant for a birdsong recording.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/formants}
\caption{Depiction of the first non-zero formant trajectories (blue) on top of a birdsong recording spectrogram of the species \emph{Periparus after}. Remark how the trajectory corresponds to the white bands (sections with a larger amount of energy) of the spectrogram.}
\label{fig_specformants}
\end{figure}

\subsection{Statistical models}
Once each birdsong recording has been transformed into a formant trajectory, we can train models that characterise each bird species as a function. Recall that this step endows our approach with the length-invariance property, which is crucial in order to capture similarities across recordings of different durations. In particular, we first describe Kernel Density Estimation to estimate probability distribution functions (pdfs) in subsubsection \ref{sub_kde}. Then, in subsubsection \ref{sub_hmms}, we present Variational Bayes Hidden Markov Models (VB-HMM), which are able to learn time-ordered data. 

\subsubsection{Kernel Density Estimation}
\label{sub_kde}
\par Kernel Density Estimation (KDE) approximates the probability density function of a set of points as a linear combination of basis functions (non-negative kernels). We place an instance of this basis function around each point in the set, add them up and normalise. For the univariate case, the pdf $p_K(x)$ obtained from KDE using kernel $K$ over a set of points $X$ is given by:
\begin{align*}
p_K(x) = \frac{1}{Nh}\sum_{i=1}^NK\left(\left|\frac{x - x_i}{h}\right|\right)
\end{align*}
where $h$ is called the bandwidth. Crucially, using a smooth kernel provides a smooth pdf as a result, where the degree of smoothness is controlled by the parameter $h$, and its value will make the resulting probability function be oversmoothed, undersmoothed or optimally smoothed.
\par One of the most common smooth kernels in the literature is the univariate Gaussian kernel \cite{hastie2008}, given by:
\begin{align*}
K_G\left(\frac{x}{h}\right) = \frac{1}{\sqrt{2\pi}}\exp{\left(-\frac{x^2}{2h^2}\right)}
\end{align*}
whose optimal bandwidth in the least-squares error sense is given by $h = \hat{\sigma}n^{-1/5}$, where $\hat{\sigma}$ is the standard deviation of the sample $X$.
\par Note that, despite its simplicity, KDE is a poorly scalable strategy: on one hand, the bandwidth $h$ becomes harder to choose as the dimensionality of data increases \cite{Hansen2009}; additionally, the space required to store the output pdf increases exponentially with each added dimension.

\subsubsection{Gaussian Mixture Models}
\label{subsection_gmm}
The scalability challenge imposed by KDE motivates the look for a more compact representation of data. Note that formant trajectories for different birdsong syllables from the same species have been shown to exist in different frequency ranges. This suggests the existence of subpopulations in our data $\mathcal{X}$. In this case, we can use a smaller number of basis functions. This can be achieved by means of a Gaussian Mixture Model (GMM), which is a convex combination of Gaussians (called \emph{mixtures}). In other words, the pdf fitted to the data takes the form:
\begin{equation} \label{eq:mixmod}
p(\V{x} \given \theta ) = \sum_{i=1}^K\pi_i \mathcal{N}(\V{\mu}_i, \Sigma_i) 
\end{equation}
with $\sum_i=1^K\pi_i, 0 \leq \pi_i\leq 1$. Nevertheless, even if GMMs are more scalable than KDE, neither method allows for the existence of time ordering of data. That is, both models treat the formant trajectories as sets, rather than sequential structures. This motivates the need for an even more abstract structure that takes into consideration the sequential structure of formant trajectories.

\subsubsection{Variational Bayes Hidden Markov Models} \label{sub_hmms}
Although Gaussian Mixture Models overcome the scalability issues of KDE, both techniques still suffer from losing the sequential structure of formant trajectories. Hidden Markov Models overcome these challenges by combining an \emph{emission model} with parameters $\phi$, a \emph{transition model}, which is a Markov chain with transition matrix $B$, and an initial state categorical distribution $\pi$, which altogether describe a sequence $\V{x}$: the emission model explains the observations in the sequence, whereas the transition model and the initial state distribution explain its time ordering.
\par In this scenario, we aim to infer the probability that the model generated such observations $\V{x}$ and that the underlying Markov Process generated the sequence of states $\V{s}$. This is given by the joint probability $p(\V{x}, \V{s})$:
\begin{equation}\label{eq:hmm}
p(\V{x}, \V{s}) = p(\V{s})p(\V{x} \given \V{s}) = p(s_1)\prod_{t=1}^{T-1}p(s_{t+1}\given s_t)\prod_{t=1}^Tp(\V{x}_t \given s_t)
\end{equation} 
\par An HMM is fully characterised (up to a permutation of states) by these three parameters $\theta = (\phi, B, \pi)$. Furthermore, the observations analysed by an HMM can be continuous or discrete \cite{Jurafsky2009}. Note that formants $\V{f}_i$ belong to a continuous space, hence we will focus on describing continuous HMMs henceforth.
\par Let the emission model of an HMM be a GMM. Notice that the transition model explains the data's sequential structure without losing any of the GMM's advantages described in section \ref{subsection_gmm}. However, note that for every HMM with $K$ states, there exist $K!$ equivalent HMMs that can be generated by permuting the identities of the HMM's states. This problem is called \emph{identifiability} \cite{Bishop2006}, and will play an important role when we define a metric between HMMs. 
\par We first address HMM training. We aim to make inferences about the parameters $\theta$ given data $\mathcal{X}$. A \emph{frequentist} approach to this is Maximum Likelihood Estimation (MLE). However, MLE has several disadvantages \cite{Murphy2012}. Firstly, there is no measure of uncertainty regarding the point estimate $\hat{\theta}$, and thus we are not able to know how trustworthy the estimate is. Secondly, the mode of a distribution is often untypical of the distribution \cite{Murphy2012} because it does not take into account the volume distribution of the probability distribution. Finally, MLE estimates are prone to overfit data. To illustrate this scenario, consider a fair coin that is tossed two times, and it lands heads on both occasions. An MLE estimate for this Bernoulli distribution would set $\hat{\theta} = 1$, i.e. it would immediately conclude that the coin is loaded \cite{Murphy2012}.
\par These disadvantages can be treated by performing Bayesian learning. In the Bayesian approach, we measure uncertainty by letting the parameters $\theta$ be probability distributions about which we have beliefs. These are represented by prior distributions. A full treatment of how this framework can be used specifically for continuous HMM training is given in \cite{Rezek2005}, where the posterior distributions for the transition and initial state are approximated as Dirichlet distributions. The authors also give an account of the emission models for different kinds of observations. In particular, for Gaussian models, they approximate the mean as a Normal Distribution and the precision matrices as Wishart densities. 
\par There is still one more issue to address: model selection, i.e. determining how many states an HMM should have. Although techniques such as cross-validation have been extensively investigated \cite{Siddiqi2007,Rezek2005}, Variational Bayes approaches have been shown to have a natural shrinkage of the number of states, i.e. the state space dimension $K$ need not be estimated by iterative training and testing of models. Instead, the system finds an optimal number of states during training: states not visited by the model collapse and only those that correspond to the true number of clusters are used \cite{Rezek2005}. 

\subsection{Similarity metrics}
We are now concerned by distance metrics between probability distributions and between HMMs, and aim to give some examples of metrics in order to build relational structures.
\par The Symmetric Kullback-Leibler divergence and the Hellinger distance are similarity metrics for continuous probability densities, which can be approximated for discrete probability distributions $P, Q$ as follows:
\begin{align*}
d_{\text{SKLD}}(P, Q) &= \frac{1}{2}\sum_iP_i\log{\frac{P_i}{Q_i}} + Q_i\log{\frac{Q_i}{P_i}}\\
d_{\text{H}}(P, Q) &= \sqrt{1 - \sum_i \sqrt{P_iQ_i}}
\end{align*}
\par These expressions allow us to compute distances between non-parametric distributions, such as those obtained from Kernel Density Estimation. However, this approach is linear ($\mathcal{O}(n)$) on the number of points in the distribution; closed forms do exist for parametric distributions, accounting for a more efficient computation, $\mathcal{O}(1)$. 
\par The literature on HMM similarity is not broad. The authors in \cite{Lyngs1999} define a similarity metric in terms of the co-emission probability (i.e. the probability of generating the same sequence of observations independently); \cite{Bahlmann2001} uses the Bayes Probability of Error instead, which can be understood as the area of overlap of two densities $p_1, p_2$ with priors $\pi_1, \pi_2$ under the restriction $\pi_1+\pi_2 = 1$; finally, a third distance measure is proposed in \cite{Juang1985} for discrete HMMs, and consists in computing the norm between their emission models, which are empirical pdfs.
\par It can be seen that the literature on HMM similarity focuses on computing similarity between emission models (potentially weighted by first-state distributions). While there is a potential loss of information as a result of not computing the similarity between transition models too, it is also true that some information still remains due to the HMM training algorithms. 
\par As a consequence, a metric between two HMMs $\lambda_p, \lambda_q$ with GMM emission models $\phi_p, \phi_q$ can also be defined as:
\begin{align*}
D_{\text{GMM}}(\lambda_p, \lambda_q) = d( \phi_p, \phi_q )
\end{align*}
Where $d$ is a similarity measure between probability distributions, such as the SKLD or the Hellinger Distance. Remark that there is no closed form to compute neither the SKLD nor the Hellinger distance between two GMMs, thus this procedure is computationally expensive.
\par The approach above, not unlike most of those that already exist in the literature, does not account for all the information provided by the transition model itself. Comparing transition models for two HMMs $\lambda_p, \lambda_q$ amounts to comparing two Markov chains. This can be done by comparing the Dirichlet distributions between pairs of states $q_p, q_r$. 
\par The first challenge that arises is related to identifiability. In particular, let $\lambda_p, \lambda_q$ be two HMMs, with $\psi_{p, k}, \psi_{q, l}$ being the Dirichlets corresponding to state $k$ of $\lambda_p$ and state $l$ of $\lambda_q$. Assume that $\lambda_p, \lambda_q$ have the same number of states $K$, then each of them has exactly $K!-1$ equivalent models up to a permutation of states. 
\par One way to solve this issue is by establishing a heuristic that maps all $K!$ equivalent models to the same one. To that end, let us define the occupancy of a state with respect to a sequence $\V{x}$. 
\begin{definition}\label{def_occupancy}
Let $\lambda$ be an HMM with $K$ states $Q = \{q_1, q_2, ..., q_K\}$, and let $\V{x}$ be a sequence of $N$ observations with most likely sequence of states $\V{z}_o$. Then, the occupancy of state $i$ with respect to $\V{x}$ is defined as:
\begin{align*}
\Omega_\lambda(i) = \frac{\sum_{i=1}^N\mathbb{I}(z_{i} = q_i)}{N}
\end{align*}
\end{definition}
\par The vector $\Omega_\lambda$ forms a discrete probability distribution over the states $Q$, and it also induces an ordering over all $K!$ equivalent HMMs. In particular, for a given sequence $\V{x}$, assume $\Omega_\lambda(i) \neq \Omega_\lambda(j), \forall i\neq j$ and define $R = \{(q_i, q_j) \given \Omega_\lambda(i) \leq \Omega_\lambda(j)\}$. Then, $R$ is:
\begin{itemize}
\item reflexive, $\Omega_\lambda(i) \leq \Omega_\lambda(i) \forall i$,
\item antisymmetric, $\Omega_\lambda(i) \leq \Omega_\lambda(j), \Omega_\lambda(j) \leq \Omega_\lambda(i) \implies i = j$ is always true by our initial assumption $\Omega_\lambda(i) \neq \Omega_\lambda(j), \forall i\neq j$,
\item and transitive $\Omega_\lambda(i) \leq \Omega_\lambda(j), \Omega_\lambda(j) \leq \Omega_\lambda(k) \implies \Omega_\lambda(i) \leq \Omega_\lambda(k)$
\end{itemize}
and thus $R$ is a partial order.
\par Now, let $\lambda_p^\text{*}$ be the HMM whose states are sorted by occupancy, i.e. $q_1^\text{*} < q_2^\text{*} < ... < q_K^\text{*}$. Notice that we can establish a bijection between any other equivalent $\lambda_p$ with states $Q = \{q_1, q_2, ..., q_K\}$ and $\lambda_p^\text{*}$ by mapping $q_i \rightarrow q_i^\text{*}$. In other words, for any two equivalent $\lambda_p, \lambda_p'$ with $\Omega_{\lambda_p}, \Omega_{\lambda_p'}$ defined over the same sequence $\V{x}$, then $\Omega_{\lambda_p} = P\Omega_{\lambda_p'}$ for some permutation matrix $P$. Intuitively, if we let $\V{z}_p, \V{z}_{p'}$ be the most likely sequences of states for the observations $\V{x}$, then we can transform $\V{z}_p$ into $\V{z}_{p'}$ simply by renaming the sequence according to $q_i \rightarrow q_i^\text{*}$. 
\par We can now define a similarity metric between any two HMMs $\lambda_p, \lambda_q$ with the same number of states, with occupancy-sorted states and assuming that no two states have the same occupancy. The distance $D_{\text{Trans}}(\lambda_p, \lambda_q)$ between two HMMs $\lambda_p, \lambda_q$ is defined as:
\begin{align*}
D_{\text{Trans}}(\lambda_p, \lambda_q) = \sum_{i=1}^K d(\psi_{p, i}, \psi_{q, i})
\end{align*}
where $\psi_{p, i}, \psi_{q, i}$ are the Dirichlet distributions associated to the transitions from state $i$ and $d$ is a probability distribution distance metric, such as the SKLD. Notice that this definition satisfies the properties of a similarity function:
\begin{itemize}
\item Symmetry. $D_{\text{Trans}}(\lambda_p, \lambda_q) = \sum_{i=1}^K d(\psi_{p, i}, \psi_{q, i}) = \sum_{i=1}^K d(\psi_{q, i}, \psi_{p, i}) = D_{\text{Trans}}(\lambda_q, \lambda_p)$
\item $D_{\text{Trans}}(\lambda_p, \lambda_p) = \sum_{i=1}^K d(\psi_{p, i}, \psi_{p, i}) = 0$
\item Triangle inequality. Using non-negativity of $d$:
\begin{align*}
D_{\text{Trans}}(\lambda_p, \lambda_r) &= \sum_{i=1}^K d(\psi_{p, i}, \psi_{r, i}) \\
&\leq \sum_{i=1}^K d(\psi_{p, i}, \psi_{q, i}) + d(\psi_{q, i}, \psi_{r, i})\\
&= D_{\text{Trans}}(\lambda_p, \lambda_q) + D_{\text{Trans}}(\lambda_q, \lambda_r)
\end{align*}
\end{itemize}
\par Nevertheless, one could still argue that a VB HMM would have some states that are seldom used would have a negative impact to obtain a faithful computation of similarity: if two states $q_i \in Q_p, q_j \in Q_q$ are very rarely used, then their similarity is not as relevant as the rest of the states in $Q_p, Q_q$. To address this issue, we introduce a weighted metric:
\begin{equation} \label{eq:hmmdist}
D_{\text{Trans}}(\lambda_p, \lambda_q) = \sum_{i=1}^K \frac{\Omega_{\lambda_p}(i) + \Omega_{\lambda_q}(i)}{2} d(\psi_{p, i}, \psi_{q, i})
\end{equation}
that combines the contribution of each pair of states according to its averaged occupancy.

\subsection{Agglomerative Hierarchical clustering}
AHC consists in initialising $N$ singleton clusters from the dataset $\mathcal{X} = \{x_1, x_2, ..., x_N\}$ and progressively merging them until only a single cluster containing all of $\mathcal{X}$ remains. Each level of the hierarchy is a subset of the dataset $\mathcal{X}$, and the number of subsets never increases over time.
\par The AHC algorithm is displayed in algorithm \ref{alg_ahc}. It comprises only two stages: a merging phase, in which the two closest clusters are merged, and a recalculation stage, in which the similarity between the newly created cluster and the rest is recalculated. How the new similarities are computed depends on the linkage method. The three most widely known linkage methods \cite{hastie2008} are described below:
\par Single linkage consists of letting the two clusters $G, H$ be as close as they can be by choosing the distance of their closest elements. In other words:
\begin{align*}
d_{SL} = \min_{i \in G, i' \in H} d_{i, i'}
\end{align*}
Similarly, the complete linkage method chooses the two elements that are the furthest apart:
\begin{align*}
d_{CL} = \max_{i \in G, i' \in H} d_{i, i'}
\end{align*}
Finally, group average clustering instead takes the average distance between each pair:
\begin{align*}
d_{GA} = \frac{1}{\abs{G}\abs{H}}\sum_{i\in G}\sum_{i'\in H}d_{i, i'}
\end{align*}
\begin{algorithm}
\begin{algorithmic}[1]
\Function{AHC}{$X, M$}
\Repeat
\State Merge the two closest clusters
\State Recalculate similarity between the new cluster and the rest
\Until{only a single cluster remains}
\EndFunction
\caption{The Agglomerative Hierarchical Clustering algorithm.}\label{alg_ahc}
\end{algorithmic}
\end{algorithm}
\par Note that all three methods tend to show the same results for data that exhibits clustering tendencies \cite{hastie2008}, i.e. tight clusters well apart from each other. The result of the AHC algorithm can be visualised in a dendrogram, which is the representation of the resulting arborescent structure. Note that AHC algorithms have a monotonicity property that always guarantees the existence of a dendrogram representation \cite{hastie2008}.

\subsubsection{Dendrogram divergence}
\label{subsub_divergence}
We also concerned by computing the similarity between the structure of two trees $T_i, T_j$. This will enable us to show that our resulting trees do have real structure because they are fundamentally different from trees generated from random data, as will be seen in subsection \ref{sub_results}.
\par Consider a tree $T$, and let $c(x)$ be the function that gives the number of clusters remaining by stopping Agglomerative Hierarchical Clustering at lifetime $x$, and let $c'(x)$ be its derivative. We now define the divergence between trees $T_i, T_j$ as:
\begin{align*}
d(T_i, T_j) = \norm{c'_i(x) - c'_j(x)}
\end{align*}
Using the derivative enables us to measure how fast clusters form as their lifetime increases; in particular, if both trees $T_i, T_j$ have a similar structure, then the rate of change of cluster formation will be similar in both cases too.

\section{Experiments and results}
\label{section_results}
\subsection{Implementation}
We implemented a system capable of building phylo-acoustic trees automatically. The implementation runs in MATLAB 2015a and all the experiments were executed on a computer with an Intel i5-3317U CPU, 8.00 GB RAM, running Windows 8.1 64-bits. 
\par With respect to the data, we used the Animal Sound Archive bird vocalisation dataset, which contains 3686 birdsong recordings of more than 100 different bird species from all over Europe \cite{AnimalSoundArchive2015}. We used the data for 82 bird species that had at least 15 seconds of birdsong recording each. All the recordings are in WAV format (i.e. uncompressed) and sampled at 22,500 Hz.
\par We used the following pre-processing before extracting formant trajectories: a pre-emphasis filtering with $\alpha = 0.95$; segmentation into frames of 40 ms with an overlap of 50\% \cite{Stowell2014}; finally, each frame is convolved with a Hamming window to avoid sharp edges. 
\par The number of formants we estimate per window corresponds to the rule of thumb $N_f = f_s / 2000$, where $f_s$ is the sampling frequency of the recording \cite{markel1976}. Since birdsong goes to frequencies as high as 10,000 Hz \cite{Marler2004}, and that we want to have 1 formant per thousand Hz, all the birdsong files must be sampled at least at 20,000 Hz, we chose a conventional 22,500 Hz sampling frequency. Furthermore, the order of the LPC model corresponds to $p = 2N_f + 2$ to account for complex conjugate roots \cite{Benesty}. 
\par Once the formants for a given window have been calculated, all values $f_{i, j} < 500$ Hz are filtered as noise \cite{Stowell2014}, and only the formants with a bandwidth narrower than 500 Hz are kept. 
\par As for Kernel Density Estimation, the function \texttt{ksdensity} is included in the MATLAB Statistics and Machine Learning Toolbox. After estimating the probability distribution of each recording, all the distributions corresponding to the same species are averaged to obtain a single, characterising distribution per species. Figure \ref{fig_kdespecies} shows the result of this procedure for 5 different bird species in the ASA dataset.
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/kdespecies}
\caption{Comparison of probability distributions generated by KDE for 5 different species. Despite having high probability in similar ranges of frequencies, their shapes are still distinguishable. }
\label{fig_kdespecies}
\end{figure}
\par Concerning VB-HMMs, we used the implementation of the Oxford Machine Learning Research Group, HMMBOX 4.1, which requires the NETLAB package. The former contains implementations of the tools required for a Bayesian treatment of HMMs, in particular: implementation of closed forms for the KL-divergence of the Normal, Dirichlet and Wishart distributions, variational inference training and sequence decoding. Furthermore, a routine that computes the occupancy of an HMM $\Omega_\lambda$ with respect to a sequence $\V{x}$ was also implemented. After computing the formant trajectories for each recording, the ones corresponding to a single species were concatenated and used as the training sequence for each HMM, i.e. we trained one HMM per species, and then computed their occupancy with respect to their respective training sequences. We used a Gaussian observation model for every HMM.
\par All the similarity metrics described in section \ref{section_model} were implemented as MATLAB scripts as well. Finally, the MATLAB Statistics and Machine Learning Toolbox contains functions to perform linkage and dendrogram plotting from a similarity matrix.

\subsection{Results}
\label{sub_results}
In this subsection, we present phylo-acoustic trees obtained from using the techniques described in section \ref{section_model}. We used 6 different approaches to build phylo-acoustic trees:
\begin{itemize}
\item Symmetric KL Divergence over non-parametric distributions generated via KDE.
\item Hellinger distance over non-parametric distributions generated via KDE.
\item Symmetric KL Divergence over Gaussian Mixture Models trained as emission models for a Hidden Markov Model.
\item Hellinger distance over Gaussian Mixture Models trained as emission models for a Hidden Markov Model.
\item Symmetric KL Divergence over the Dirichlet distributions associated to the transition models of our Hidden Markov Models.
\item Occupancy-weighted Symmetric KL Divergence (as described in subsection \ref{sub_hmms}) over the Dirichlet distributions associated to the transition models of our Hidden Markov Models.
\end{itemize}
\par We first show that all these trees have true structure by 
disproving the null hypothesis:
\begin{align*}
H_0 = \text{there is no relationship between the species in a phylo-acoustic tree.}
\end{align*}
\par If this hypothesis is true, then randomly generated data has the same structure as any of these trees. To generate such random data, we sample formant trajectories from the uniform distribution $\mathcal{U}(f_{min}, f_{max})$, where the boundaries of the distribution correspond to the minimum and maximum formant seen in any species in the dataset. After generating as many formant trajectories as bird species, we build new ``phylo-acoustic trees'' for each of them.
\par We can now decide which tree exhibits the most structure by computing the divergence (see subsection \ref{subsub_divergence}) between the trees $T_i, T_{i'}$, which have been generated by using method $i$ from the list above on real data and random data, respectively. The divergence gives a measure on how similar the rates of change between both trees are, hence a tree with a higher divergence from its random counterpart exhibits a clearer structure. The divergences for all six pairs of phylo-acoustic trees are shown in Table \ref{tab1}.

\begin{table}[t]
\begin{tabular}{r|r|r}
     \textbf{method} &\textbf{divergence}\\
     \hline
     \textbf{SKLD over pdfs via KDE} & 6704\\
     Hellinger over pdfs via KDE & 1188\\
     \textbf{SKLD over GMMs from VB-HMM} & 2574\\
     Hellinger over GMMs from VB-HMM & 406\\
     SKLD over Dirichlets from VB-HMM & 356\\
     Occupancy-weighted SKLD over Dirichlets from VB-HMM & 472
\end{tabular} 
\caption{Divergence between pairs of trees obtained from real data and random data, respectively. The greater the value of the divergence is, the clearer the structure the real data tree has.} \label{tab1}
\end{table}

\par Full-size figures of the two phylo-acoustic tree with the largest divergences are offered below. Brown marks have been added to each figure whenever more than 40\% of a cluster consisted of species from the same genus.

\begin{sidewaysfigure}[!ht]
\noindent\makebox[\textwidth]{\includegraphics[width=\paperheight,height=18cm]{images/kde_skld_2}}
    \caption{Phylo-acoustic tree generated using the Symmetric KL Divergence between pairs of non-parametric distributions generated using KDE.}
    \label{fig:kdeskld}
\end{sidewaysfigure}

\begin{sidewaysfigure}[!ht]
\noindent\makebox[\textwidth]{\includegraphics[width=\paperheight,height=18cm]{images/gmm_skld_2}}
    %\includegraphics[width=\textwidth]{gmm_skld_2}
    \caption{Phylo-acoustic tree generated using the Symmetric KL-Divergence between pairs of emission models (GMMs) from HMMs.}
    \label{fig:gmmskld}
\end{sidewaysfigure}

\par These trees can be assessed in several ways - choosing to mark clusters whose species share an ancestor is just one of them. The versatility of this approach suggests that, rather than proving that facts from Zoology extend to Acoustic Analysis, we can instead show acoustic relations that do exist and then open a discussion about why they exist.
\par This key point was briefly discussed in section \ref{section_introduction}, when we argued that birds sharing common ancestors need not necessarily share birdsong traits in the future. Birdsong is not uniquely determined by genes, instead it often suffers modifications that are due to mating advantage and adaptation to new environments. For this reason, relations unveiled in phylo-acoustic trees could also be linked to migration patterns and, more generally, geolocalisation.
\par Nevertheless, one of the most important conclusions we aim to draw from these trees is that their structure is not random. When there is real structure in data, clusters are tight and well separated from each other. This implies the distance between clusters becomes larger as smaller clusters merge. This maps visually to long branches in a dendrogram. On the other hand, when there is no real structure in data, objects tend to be about the same distance apart from each other (they are uniformly scattered in space). When a new cluster is formed, linkage methods tend to change very little the distances between the original clusters, since they were very similar before merging. This translates into clusters that have a short lifetime.
\par In order to compare the structure of trees obtained from real and random data, we plot the number of clusters $c(x)$ in each tree obtained after running Hierarchical Clustering up to lifetime $x$. This is a monotone decreasing function that is useful to contrast how quickly do clusters form as lifetime increases. As an example, see Figure \ref{fig_lifetime}.
\par We generated one such plot for each pair of trees $T_i, T_{i'}$. In all six cases, we noted that the curves depicting the random statistical models drop to zero very quickly, since the resulting clusters live too little, as opposed to the real data models, whose clusters live many orders of magnitude longer. 


\clearpage

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/lifetime_gmm_skld}
\caption{Lifetime vs. number of clusters using the Symmetric KL Divergence between non-parametric distributions obtained via KDE. Note the rate of change of the red curve (representing random structures) compared with the blue one (real data). The red curve approaches zero very quickly, suggesting that clusters are merged at a fast rate; on the other hand, the blue lines do not only span a longer lifetime, but they also contain short horizontal segments that grow longer as time flows. This suggests that it takes longer for clusters to merge as the number of remaining clusters decreases, which is consistent with our hypothesis: at the beginning, many clusters belong to the same community, and thus they are clustered fairly quickly. However, as communities are formed, they lie further apart from each other, and thus it takes longer for them to fuse.}
\label{fig_lifetime}
\end{figure}



\section{Conclusion}
\label{section_conclusion}
In this work, we presented an automated phylo-acoustic tree generating algorithm and its software implementation. These results were not aimed at finding a clear mapping between phylo-acoustic and phylo-genetic trees, but rather as an exploratory means that unveils interesting acoustic relationships among species, some of which are related to genetic links. However, we must also account for the fact that a wide variety of phenomena affect how birdsong evolves, e.g. adaptation of migrating bird flocks, environmental pressures and mating advantage all affect how birdsong changes over time. 
\par Nevertheless, some relations are in fact related to phylogenetic links. For example, consider the \emph{Phylloscopus} genus. In all six phylo-acoustic trees shown in last section, species belonging to this genus tend to cluster together, suggesting the traits they share can be identified even by means of different techniques.
\par Another example are the genera \emph{Phylloscopus}, \emph{Sylvia} and \emph{Acrocephalus}. Species belonging to these are often clustered together - sometimes even before being grouped with other species of their same genus. This comes to no surprise when we discover that all three genera are warblers in English, suggesting that the tree does show some connection to phylo-genetic relations. Further discussion regarding this kind of observations is outside the scope of this work.
\par In conclusion, we have described a method to automatically build phylo-acoustic trees from birdsong in a length-invariant fashion. This enables users to visualise interactions between bird species, and give a meaningful interpretation to them, which could be justified by a real-world phylo-genetic, environmental or behavioural relation. Crucially, we characterised bird species as statistical models, which allowed for length-invariant comparison between them. The resulting phylo-acoustic trees were shown to have a real community structure (from the mathematical perspective) and, at the same time, to display interactions that do take place in the world (e.g. clustering species that belong to the same genus).

\section*{Acknowledgements}
The authors would like to thank the National Council for Science and Technology in Mexico (CONACYT) for funding this research project. 


\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{unsrt}
\bibliography{bib}

\end{document}


